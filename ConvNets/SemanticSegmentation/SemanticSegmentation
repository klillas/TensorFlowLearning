import tensorflow as tf
import numpy as np
#impot scikit as sc
from ConvNets.SemanticSegmentation import SemanticSegmentationData


class SemanticSegmentation:
    data_train = None

    tf_ph_x = None
    tf_ph_labels = None

    tf_tensor_cost = None
    tf_tensor_model = None

    hyper_param_width = None
    hyper_param_height = None
    hyper_param_label_size = None

    def initialize(self, data_train: SemanticSegmentationData, width, height):
        self.hyper_param_width = width
        self.hyper_param_height = height

        self.data_train = data_train

        self.tf_ph_x = tf.placeholder(tf.float32, [None, self.hyper_param_width, self.hyper_param_height, 1], name="x")
        self.tf_ph_labels = tf.placeholder(tf.int32, [None, self.hyper_param_width, self.hyper_param_height, self.hyper_param_label_size], name="labels")

        self._initialize_model()
        self._initialize_cost()

    def _initialize_model(self):
        model = tf.layers.conv2d(
            inputs=self.tf_ph_x,
            filters=16,
            kernel_size=[4, 4],
            padding="same",
            activation=tf.nn.relu
        )

        model = tf.layers.max_pooling2d(
            inputs=model,
            pool_size=[2, 2],
            strides=2
        )

    def _initialize_cost(self):
        # Calculate distance from actual labels using cross entropy
        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.tf_tensor_model, labels=self.tf_ph_labels)
        # Take mean for total loss
        self.tf_tensor_cost = tf.reduce_mean(cross_entropy, name="fcn_loss")